{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_pQCcdq1E5_l"
   },
   "source": [
    "**EEP523 Spring 2020. Mobile Applications for Sensing and Control**\n",
    "\n",
    "*April 30th, 2020*\n",
    "\n",
    "**Assignment 4: Practical Introduction to running CNN \n",
    "on an Android App using Tensor Flow Lite.\n",
    "Custom Gesture Recognition App**\n",
    "\n",
    "Refer to assignment pdf document for instructions\n",
    "\n",
    "This colab will be used to download the required files (python files and text files with the dataset) you need for the assignment, and run the python files on a remote GPU machine with TensorFlow. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-gpu in c:\\users\\chuck\\appdata\\roaming\\python\\python37\\site-packages (2.1.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.9.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.33.6)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.11.2)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (3.11.3)\n",
      "Requirement already satisfied: gast==0.2.2 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.2.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (3.1.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.16.5)\n",
      "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (2.1.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.1.8)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.0.8)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.12.0)\n",
      "Requirement already satisfied: tensorflow-gpu-estimator<2.2.0,>=2.1.0rc0 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.27.2)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.4.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.1.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.8.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.1.0)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda3\\lib\\site-packages (from protobuf>=3.8.0->tensorflow-gpu) (41.4.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in d:\\anaconda3\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.16.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\anaconda3\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.2.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in d:\\anaconda3\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.14.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in d:\\anaconda3\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\anaconda3\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2.22.0)\n",
      "Requirement already satisfied: h5py in d:\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.8->tensorflow-gpu) (2.9.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in d:\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (4.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in d:\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (4.1.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in d:\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in d:\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.24.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in d:\\anaconda3\\lib\\site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8753926534702887477\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 9456916542445467130\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# for gpu in gpus:\n",
    "#     print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)\n",
    "\n",
    "# # tf.test.is_gpu_available()\n",
    "# tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xaUtSyWnCsFU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'git' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Clone the repository from GitHub\n",
    "!git clone --depth 1 -q https://gitlab.cs.washington.edu/arjonal/eep523.git\n",
    "# Copy the training scripts into your workspace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lHr2h12dE732"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/chuck/Downloads/New folder/eep523/Assignment4\n"
     ]
    }
   ],
   "source": [
    "%cd eep523/Assignment4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G1y6HLG0BI51"
   },
   "source": [
    "The file with the complete wing gesture data contains the gesture demonstrated by a named individual. There are approximately 15 individual performances of the wing gesture. Each performance consists of a log of up to a few seconds’ worth of data. The gesture itself occurs at some point within that window, with the device being held still for the remainder of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iER_TP89FI89"
   },
   "source": [
    "In addition to the users thata, there is some generated synthetic data. This is a term for data that is generated algorithmically, rather than being captured from the real world. The synthetic data is equivalent to movement of the accelerometer that doesn’t correspond to any particular gesture. This data is used to train our “unknown” category. Because creating synthetic data is much faster than capturing real-world data, it’s useful to help augment our training process. However, real-world variation is unpredictable, so it’s not often possible to create an entire dataset from synthetic data. In our case, it’s helpful for making our “unknown” category more robust, but it wouldn’t be helpful for classifying the known gestures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ezwh5QDwFhzM"
   },
   "source": [
    "## Split dataset: train, validation, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "btP8hIgPFfDI"
   },
   "source": [
    "This script splits the data into training, validation, and test sets. Because our data is labeled with the person who created it, we’re able to use one set of people’s data for training, another set for validation, and a final set for test. The data is split as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4odUsMasF0YQ"
   },
   "source": [
    "We use six people’s data for training, two for validation, and two for testing. In addition, we mix in our negative data, which isn’t associated with a particular user. Our total data is split between the three sets at a ratio of roughly 60%/20%/20%, which is pretty standard for machine learning.\n",
    "\n",
    "In splitting by person, we’re trying to ensure that our model will be able to generalize to new data. Because the model will be validated and tested on data from individuals who were not included in the training dataset, the model will need to be robust against individual variations in how each person performs each gesture.It’s also possible to split the data randomly, instead of by person.\n",
    "\n",
    " In this case, the training, validation, and testing datasets would each contain some samples of each gesture from every single individual. The resulting model will have been trained on data from every single person rather than just six, so it will have had more exposure to people’s varying gesturing styles. However, because the validation and training sets also contain data from every individual, we’d have no way of testing whether the model is able to generalize to new gesturing styles that it has not seen before. \n",
    " \n",
    " A model developed in this way might report higher accuracy during validation and testing, but it would not be guaranteed to work as well with new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pzgxCaglE_j3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_length:581\r\n",
      "train_length:379\r\n",
      "valid_length:96\r\n",
      "test_length:106\r\n"
     ]
    }
   ],
   "source": [
    "!python data_split_person.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V4rrQLPary4F"
   },
   "source": [
    "## Load TensorBoard\n",
    "\n",
    "Now, we set up TensorBoard so that we can graph our accuracy and loss as training proceeds.TensorBoard will help us monitor the training process.\n",
    "\n",
    "Training logs will be written to the logs/scalars subdirectory of the training scripts’ directory, so we pass this in to TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yiEv9dXhrkiQ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b2b192bea28b3def\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b2b192bea28b3def\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t9tOBz6esAnl"
   },
   "source": [
    "## Begin training\n",
    "\n",
    "The following cell will begin the training process. Training will take around 5 minutes on a GPU runtime. You'll see the metrics in TensorBoard after a few epochs.\n",
    "\n",
    "The `train.py` script writes a model, `model_name.tflite`, to the training scripts' directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VUC030ggrbT4"
   },
   "source": [
    "Training is much faster using GPU acceleration. Before you proceed, ensure you are using a GPU runtime by going to Runtime -> Change runtime type and selecting GPU. Training will take around 5 minutes on a GPU runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oLE9v2T8sCdh",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to load data...\n",
      "train_data_length:9475\n",
      "valid_data_length:96\n",
      "test_data_length:106\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Built CNN.\n",
      "Start training...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 128, 3, 8)         104       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 42, 1, 8)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 42, 1, 8)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 42, 1, 16)         528       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 1, 16)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 1, 16)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                3600      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 136       \n",
      "=================================================================\n",
      "Total params: 4,368\n",
      "Trainable params: 4,368\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model size: 17.0625 KB\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 152, in <module>\n",
      "    test_len, test_data)\n",
      "  File \"train.py\", line 89, in train_net\n",
      "    for data, label in test_data:  \n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 2034, in __iter__\n",
      "    return iter(self._dataset)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 343, in __iter__\n",
      "    raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "RuntimeError: __iter__() is only supported inside of tf.function or when eager execution is enabled.\n"
     ]
    }
   ],
   "source": [
    "!python train.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zvAfh4vHKhH3"
   },
   "source": [
    "**Training metrics**\n",
    "\n",
    "**loss**\n",
    "\n",
    "This is the output of our loss function. We’re using mean squared error, which is expressed as a positive number. Generally, the smaller the loss value, the better, so this is a good thing to watch as we evaluate our network.\n",
    "\n",
    "**mae**\n",
    "\n",
    "This is the mean absolute error of our training data. It shows the average difference between the network’s predictions and the expected y values from the training data.\n",
    "\n",
    "\n",
    "**val_loss**\n",
    "\n",
    "This is the output of our loss function on our validation data. In our final epoch, the training loss could be slightly lower than the validation loss. This would be a hint that our network might be overfitting, because it is performing worse on data it has not seen before.\n",
    "\n",
    "\n",
    "**val_mae**\n",
    "\n",
    "This is the mean absolute error for our validation data. If its value is worse than the mean absolute error on our training set, it would be another sign that the network might be overfitting.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e4xlTC68G_zJ"
   },
   "source": [
    " **confusion matrix**\n",
    "\n",
    " helpful tool for evaluating the performance of classification models. It shows how well the predicted class of each input in the test dataset agrees with its actual value.\n",
    " \n",
    "Each column of the confusion matrix corresponds to a predicted label, in order (“wing” ,“unknown”). Each row, from the top down, corresponds to the actual label. From our confusion matrix, we can see that the vast majority of predictions agree with the actual labels. \n",
    "\n",
    "The confusion matrix gives us an idea of where our model’s weak points are."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assign4_GestureRecognition.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
